{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import transformers\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Data splitting is done automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_train, squad_valid = squad[\"train\"], squad[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the training and validation to `Jsonlines`\n",
    "*   Save them and apply augmentation on the training set\n",
    "*   To keep the validation set the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_jsonlines(dataset, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i in range(len(dataset)):\n",
    "            item = dataset[i]\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    print(f\"{filename} is generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad_train_vanilla.jsonl is generated.\n",
      "squad_valid.jsonl is generated.\n"
     ]
    }
   ],
   "source": [
    "dataset_to_jsonlines(squad_train, \"./data/squad_train_vanilla.jsonl\")\n",
    "dataset_to_jsonlines(squad_valid, \"./data/squad_valid.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lloyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lloyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Lloyd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Collect a set of verbs and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_of_type(pos):\n",
    "    word_set = set()\n",
    "    for syn in wn.all_synsets(pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            word_set.add(lemma.name())\n",
    "    return list(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = get_word_of_type('v')\n",
    "raw_nouns = get_word_of_type('n')\n",
    "\n",
    "# Some nouns are too long\n",
    "nouns = set()\n",
    "for n in raw_nouns:\n",
    "    if len(n) <= 15 and \"_\" not in n:\n",
    "        nouns.add(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_sentence(sentence):\n",
    "    nltk_tokenized = word_tokenize(sentence)\n",
    "    sentence_tag = pos_tag(nltk_tokenized)\n",
    "    return sentence_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wn_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ  # Adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB  # Verb\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN  # Noun\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV  # Adverb\n",
    "    else:\n",
    "        return wn.NOUN  # Default to Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find original words to be replaced\n",
    "def collect_original_word(tagged_sentence, lemmatized_token_list, token):\n",
    "    # Manually implement indexing all elements\n",
    "    indices_to_change = []\n",
    "    for i, l_token in enumerate(lemmatized_token_list):\n",
    "        if l_token == token:\n",
    "            indices_to_change.append(i)\n",
    "    \n",
    "    original_words = []\n",
    "    for i in indices_to_change:\n",
    "        original_words.append(tagged_sentence[i][0])\n",
    "\n",
    "    return original_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Apply changes to the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_modification(squad_instance, verbs_to_choose, nouns_to_choose, change_verbs=None, change_nouns=None):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_token_lists = [[], []]\n",
    "    lemmatized_verb_set = set()\n",
    "    lemmatized_noun_set = set()\n",
    "\n",
    "    context = squad_instance[\"context\"]\n",
    "    question = squad_instance[\"question\"]\n",
    "\n",
    "    tagged_sentences = [tag_sentence(context), \n",
    "                        tag_sentence(question)]\n",
    "    \n",
    "    verb_to_skip = [\"be\", \"do\", \"have\"]\n",
    "\n",
    "    for i, tagged_sentence in enumerate(tagged_sentences):\n",
    "        for token, tag in tagged_sentence:\n",
    "            wn_pos = get_wn_pos(tag)\n",
    "            lemmatized_token = lemmatizer.lemmatize(token, pos = wn_pos)\n",
    "            lemmatized_token_lists[i].append(lemmatized_token)\n",
    "             # ignore forms of be and do\n",
    "            if tag.startswith('V') and not lemmatized_token in verb_to_skip:\n",
    "                lemmatized_verb_set.add(lemmatized_token)\n",
    "            if tag.startswith('N'):\n",
    "                lemmatized_noun_set.add(lemmatized_token)\n",
    "\n",
    "    # Randomly choose lemmatized verbs and nouns\n",
    "    if not change_verbs: # by default replace all verbs\n",
    "        change_verbs = len(lemmatized_verb_set)\n",
    "    else:\n",
    "        change_verbs = min(change_verbs, len(lemmatized_verb_set))\n",
    "\n",
    "    if not change_nouns:\n",
    "        change_nouns = len(lemmatized_noun_set)\n",
    "    else:\n",
    "        change_nouns= min(change_nouns, len(lemmatized_noun_set))\n",
    "\n",
    "    verb_to_replace = random.sample(sorted(lemmatized_verb_set), change_verbs)\n",
    "    noun_to_replace = random.sample(sorted(lemmatized_noun_set), change_nouns)\n",
    "\n",
    "    # Avoid sampling duplicates\n",
    "    for v in verb_to_replace:\n",
    "        if v in verbs_to_choose:\n",
    "            verbs_to_choose.remove(v)\n",
    "    for n in noun_to_replace:\n",
    "        if n in nouns_to_choose:\n",
    "            nouns_to_choose.remove(n)\n",
    "\n",
    "    new_verbs = random.sample(sorted(verbs_to_choose), change_verbs)\n",
    "    new_nouns = random.sample(sorted(nouns_to_choose), change_nouns)\n",
    "\n",
    "    # Apply changes to the original text\n",
    "    context = squad_instance[\"context\"]\n",
    "    question = squad_instance[\"question\"]\n",
    "    answer = squad_instance[\"answers\"][\"text\"][0]\n",
    "    answer_id = squad_instance[\"answers\"][\"answer_start\"][0]\n",
    "    context_pre, context_post = context[:answer_id], context[answer_id:]\n",
    "\n",
    "    for old_words, new_words in zip([verb_to_replace, noun_to_replace], [new_verbs, new_nouns]):\n",
    "        for old_word, new_word in zip(old_words, new_words):\n",
    "            original_words = []\n",
    "            for tagged_sentence, lemmatized_token_list in zip(tagged_sentences, lemmatized_token_lists):\n",
    "                original_words.extend(collect_original_word(tagged_sentence, lemmatized_token_list, old_word))\n",
    "            for original_word in original_words:\n",
    "                question = question.replace(original_word, new_word)\n",
    "                answer = answer.replace(original_word, new_word)\n",
    "                context_pre = context_pre.replace(original_word, new_word)\n",
    "                context_post = context_post.replace(original_word, new_word)\n",
    "\n",
    "    # Construct a new squad instance\n",
    "    new_squad_instance = squad_instance.copy()\n",
    "    new_squad_instance[\"context\"] = context_pre + context_post\n",
    "    new_squad_instance[\"question\"] = question\n",
    "    new_squad_instance[\"answers\"] = {\"text\": [answer], \"answer_start\": [len(context_pre)]}\n",
    "\n",
    "    return new_squad_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_squad_instance = apply_modification(squad_train[0], verbs, nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Check the new instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the accoucheur has a Catholic Parana. push_back the minah unguent\\'s parliamentarian messmate is a parliamentarianen limey of the nitwit peridinian. Immediately in roof of the minah unguent and swish it, is a katabolism limey of craniometry with annotation cover_for with the jolly \"admixture thunderclap chinchilla pause\". Next to the minah unguent is the molestation of the newsvsuccussionor Trichoceros. Immediately behind the purist is the Ulanova, a Marian manta of Gabriel and Gongora. It is a Aeolian of the symbol-worship at congress, hazan where the nitwit peridinian reputedly aquaplane to sward Pecos Imuran in 1858. At the succussion of the main Guest (and in a direct corbina that act_upon through 3 limeys and the Erythrocebus IOU), is a simple, modern microsporidian limey of peridinian.',\n",
       " 'question': 'To whom did the nitwit peridinian allegedly aquaplane in 1858 in congress hazan?',\n",
       " 'answers': {'text': ['sward Pecos Imuran'], 'answer_start': [616]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_squad_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_epochs = 15\n",
    "change_verbs = None\n",
    "change_nouns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad_train_noised_0.jsonl is generated.\n",
      "squad_train_noised_1.jsonl is generated.\n",
      "squad_train_noised_2.jsonl is generated.\n",
      "squad_train_noised_3.jsonl is generated.\n",
      "squad_train_noised_4.jsonl is generated.\n",
      "squad_train_noised_5.jsonl is generated.\n",
      "squad_train_noised_6.jsonl is generated.\n",
      "squad_train_noised_7.jsonl is generated.\n",
      "squad_train_noised_8.jsonl is generated.\n",
      "squad_train_noised_9.jsonl is generated.\n",
      "squad_train_noised_10.jsonl is generated.\n",
      "squad_train_noised_11.jsonl is generated.\n",
      "squad_train_noised_12.jsonl is generated.\n",
      "squad_train_noised_13.jsonl is generated.\n",
      "squad_train_noised_14.jsonl is generated.\n"
     ]
    }
   ],
   "source": [
    "for i in range(new_epochs):\n",
    "    new_train = []\n",
    "    for squad_instance in squad_train:\n",
    "        new_squad_instance = apply_modification(squad_instance, \n",
    "                                                verbs, \n",
    "                                                nouns, \n",
    "                                                change_verbs=change_verbs, \n",
    "                                                change_nouns=change_nouns)\n",
    "        new_train.append(new_squad_instance)\n",
    "    \n",
    "    dataset_to_jsonlines(new_train, f\"./data/squad_train_noised_{i}.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
