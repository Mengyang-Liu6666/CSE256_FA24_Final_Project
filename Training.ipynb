{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {device}')\n",
    "\n",
    "data_path = \"./data\"\n",
    "model_path = \"./models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_jsonlines(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as json_file:\n",
    "        dataset = [json.loads(line) for line in json_file]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model: BERT<sub>size</sub>\n",
    "*   Choose size from:\n",
    "    *   Tiny\n",
    "    *   Mini\n",
    "    *   Small\n",
    "    *   Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"tiny\"\n",
    "# model_size = \"mini\"\n",
    "# model_size = \"small\"\n",
    "# model_size = \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, AutoModel\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('csarron/bert-base-uncased-squad-v1')\n",
    "model = AutoModel.from_pretrained(f\"prajjwal1/bert-{model_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Take a look at the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 128)\n",
       "    (token_type_embeddings): Embedding(2, 128)\n",
       "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-1): 2 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Take a look at input requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(model.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Create a wrapper for SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForSQuAD(nn.Module):\n",
    "    def __init__(self, bert_model=None):\n",
    "        super(BERTForSQuAD, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.qa_outputs = None\n",
    "        if bert_model:\n",
    "            self.qa_outputs = nn.Linear(bert_model.config.hidden_size, 2, bias=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "        logits = self.qa_outputs(last_hidden_state)  # Shape: [batch_size, sequence_length, 2]\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # Split into start and end logits\n",
    "        start_logits = start_logits.squeeze(-1)  # Shape: [batch_size, sequence_length]\n",
    "        end_logits = end_logits.squeeze(-1)      # Shape: [batch_size, sequence_length]\n",
    "\n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        self.bert.save_pretrained(f\"{save_path}/bert-pod\")\n",
    "        torch.save(self.qa_outputs.state_dict(), f\"{save_path}/linear_adapter.pth\")\n",
    "        torch.save(self.state_dict(), f\"{save_path}/full_model.pth\")\n",
    "    \n",
    "    def load(self, load_path):\n",
    "        self.bert = AutoModel.from_pretrained(f\"{load_path}/bert-pod\")\n",
    "        self.qa_outputs = nn.Linear(self.bert.config.hidden_size, 2, bias=True)\n",
    "        self.qa_outputs.load_state_dict(torch.load(f\"{load_path}/linear_adapter.pth\", weights_only=True))\n",
    "        self.load_state_dict(torch.load(f\"{load_path}/full_model.pth\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_squad = BERTForSQuAD(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-tiny classifier contains 4386178 parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in bert_squad.parameters())\n",
    "print(f'BERT-tiny classifier contains {total_params} parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Retrieve data in the form of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_lists(squad_train):\n",
    "    question_list = []\n",
    "    context_list = []\n",
    "    answer_start_list = []\n",
    "    answer_end_list = []\n",
    "\n",
    "    for squad_instance in squad_train:\n",
    "        question_list.append(squad_instance[\"question\"])\n",
    "        context_list.append(squad_instance[\"context\"])\n",
    "\n",
    "        answer_start = squad_instance[\"answers\"][\"answer_start\"][0]\n",
    "        answer_end = answer_start + len(squad_instance[\"answers\"][\"text\"][0])\n",
    "        \n",
    "        answer_start_list.append(answer_start)\n",
    "        answer_end_list.append(answer_end)\n",
    "\n",
    "    return question_list, context_list, answer_start_list, answer_end_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Tokenize inputs and convert the starting index of characters to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_squad_data(squad_train, tokenizer):\n",
    "    question_list, context_list, answer_start_list, answer_end_list = retrieve_lists(squad_train)\n",
    "    # Merging the question and answers together\n",
    "    tokenized_data = tokenizer(question_list, context_list, padding = True, truncation = True)\n",
    "\n",
    "    start_token_list = []\n",
    "    end_token_list = []\n",
    "\n",
    "    for i in range(len(squad_train)):\n",
    "    # Skip the question section by setting `sequence_index`=1\n",
    "        start_token = tokenized_data.char_to_token(i, answer_start_list[i], 1)\n",
    "        end_token = tokenized_data.char_to_token(i, answer_end_list[i] - 1, 1)\n",
    "\n",
    "        # if return is None, the answer passage containing the answer is truncated.\n",
    "        if start_token is None:\n",
    "            start_token_list.append(tokenizer.model_max_length-1)\n",
    "        else:\n",
    "            start_token_list.append(start_token)\n",
    "        if end_token is None:\n",
    "            end_token_list.append(tokenizer.model_max_length-1)\n",
    "        else:\n",
    "            end_token_list.append(end_token)\n",
    "        \n",
    "\n",
    "    tokenized_data.update({'answer_start':start_token_list, 'answer_end':end_token_list})\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Create a `Dataset` and convert it to a `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class squad_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_index_data = {}\n",
    "\n",
    "        for key, val in self.tokenized_data.items():\n",
    "            tokenized_index_data.update({key: torch.tensor(val[idx])})\n",
    "\n",
    "        return tokenized_index_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data.answer_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Create a function to create dataloaders from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_from_filename(filename, batch_size):\n",
    "    squad_jsonl = load_from_jsonlines(filename)\n",
    "    tokenized = tokenize_squad_data(squad_jsonl, tokenizer)\n",
    "    dataset = squad_dataset(tokenized)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Compute loss and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(start_logits, end_logits, answer_start, answer_end):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fn(start_logits, answer_start)\n",
    "    end_loss = loss_fn(end_logits, answer_end)\n",
    "    return (start_loss + end_loss) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(start_logits, end_logits, answer_start, answer_end):\n",
    "    batch_size = start_logits.shape[0]\n",
    "    \n",
    "    pred_start_list = torch.argmax(start_logits, dim=1)\n",
    "    pred_end_list = torch.argmax(end_logits, dim=1)\n",
    "    \n",
    "    F1_list = []\n",
    "    EM_list = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pred_start, pred_end = pred_start_list[i].item(), pred_end_list[i].item()\n",
    "        true_start, true_end = answer_start[i].item(), answer_end[i].item()\n",
    "\n",
    "        # Make sure the answer is valid\n",
    "        if pred_start > pred_end:\n",
    "            pred_end = pred_start\n",
    "\n",
    "        # Calculate F1-score\n",
    "        pred_tokens = set(range(pred_start, pred_end + 1))\n",
    "        true_tokens = set(range(true_start, true_end + 1))\n",
    "\n",
    "        common_tokens = pred_tokens.intersection(true_tokens)\n",
    "\n",
    "        if len(common_tokens) == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            precision = len(common_tokens) / len(pred_tokens)\n",
    "            recall = len(common_tokens) / len(true_tokens)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        F1_list.append(f1)\n",
    "\n",
    "        # Calculate Exact Match (EM)\n",
    "        if (pred_start == true_start and pred_end == true_end):\n",
    "            EM_list.append(1)\n",
    "        else:\n",
    "            EM_list.append(0)\n",
    "\n",
    "    # Sum and average later\n",
    "    return F1_list, EM_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Write a function for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    f1_list = []\n",
    "    em_list = []\n",
    "\n",
    "    for batch in valid_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        answer_start = batch['answer_start'].to(device)\n",
    "        answer_end = batch['answer_end'].to(device)\n",
    "\n",
    "        start_logits, end_logits = model(input_ids, \n",
    "                                         attention_mask=attention_mask, \n",
    "                                         token_type_ids=token_type_ids)\n",
    "        \n",
    "        loss = compute_loss(start_logits, end_logits, answer_start, answer_end)\n",
    "        f1_sublist, em_sublist = compute_metric(start_logits, end_logits, answer_start, answer_end)\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        f1_list.extend(f1_sublist)\n",
    "        em_list.extend(em_sublist)\n",
    "    \n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    f1 = sum(f1_list) / len(f1_list) * 100\n",
    "    em = sum(em_list) / len(em_list) * 100\n",
    "\n",
    "    return loss, f1, em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(model, optimizer, train_loader_list, train_schedule, valid_loader, epochs=1, save_path = f\"{model_path}model\"):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    last_valid_loss = torch.inf\n",
    "    model_to_save = copy.deepcopy(model)\n",
    "    model_to_save.load_state_dict(model.state_dict())\n",
    "    for i in range(epochs):\n",
    "        train_idx = train_schedule[i % len(train_schedule)]\n",
    "        loop = train_loader_list[train_idx]\n",
    "        loss_list = []\n",
    "        for batch in tqdm(loop):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            answer_start = batch['answer_start'].to(device)\n",
    "            answer_end = batch['answer_end'].to(device)\n",
    "\n",
    "            start_logits, end_logits = model(input_ids, \n",
    "                                             attention_mask=attention_mask, \n",
    "                                             token_type_ids=token_type_ids)\n",
    "\n",
    "            loss = compute_loss(start_logits, end_logits, answer_start, answer_end)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            \n",
    "\n",
    "        train_loss = sum(loss_list) / len(loss_list)\n",
    "        valid_loss, f1, em = valid_model(model, valid_loader)\n",
    "        print(f\"Epoch {i+1}, train_loss: {train_loss:.3f}, valid_loss: {valid_loss:.3f}, EM: {em:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "        if loss < last_valid_loss:\n",
    "            model_to_save = copy.deepcopy(model)\n",
    "            model_to_save.load_state_dict(model.state_dict())\n",
    "    \n",
    "    # Save model\n",
    "    model_to_save.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Set up learning rates and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = dataloader_from_filename(f\"{data_path}/squad_valid.jsonl\", batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vanilla = AutoModel.from_pretrained(f\"prajjwal1/bert-{model_size}\")\n",
    "bert_squad = BERTForSQuAD(model_vanilla)\n",
    "train_loader_list = [dataloader_from_filename(f\"{data_path}/squad_train_vanilla.jsonl\", \n",
    "                                              batch_size=4)]\n",
    "train_schedule = [0] # indices in the train_loader_list\n",
    "\n",
    "learning_rate = 5e-5\n",
    "adam = torch.optim.Adam(bert_squad.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:26<00:00, 82.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 3.057, valid_loss: 2.448, EM: 22.564, F1: 37.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:30<00:00, 80.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss: 2.063, valid_loss: 2.238, EM: 26.405, F1: 42.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:31<00:00, 80.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train_loss: 1.680, valid_loss: 2.260, EM: 26.868, F1: 43.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:31<00:00, 80.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train_loss: 1.376, valid_loss: 2.434, EM: 24.948, F1: 41.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:35<00:00, 79.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, train_loss: 1.102, valid_loss: 2.687, EM: 24.210, F1: 40.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:37<00:00, 78.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, train_loss: 0.853, valid_loss: 2.993, EM: 23.406, F1: 39.397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:40<00:00, 78.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, train_loss: 0.638, valid_loss: 3.594, EM: 21.485, F1: 37.283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:40<00:00, 78.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, train_loss: 0.467, valid_loss: 4.308, EM: 20.908, F1: 36.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:37<00:00, 78.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, train_loss: 0.341, valid_loss: 4.745, EM: 20.009, F1: 35.564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:39<00:00, 78.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train_loss: 0.257, valid_loss: 5.307, EM: 18.675, F1: 34.513\n"
     ]
    }
   ],
   "source": [
    "torch_train(model=bert_squad, \n",
    "            optimizer=adam, \n",
    "            train_loader_list=train_loader_list,\n",
    "            train_schedule=train_schedule,\n",
    "            valid_loader=valid_loader,\n",
    "            epochs=10,\n",
    "            save_path=f\"{model_path}/vanilla_finetuning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
