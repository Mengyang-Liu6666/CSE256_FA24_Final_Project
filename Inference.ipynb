{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {device}')\n",
    "\n",
    "data_path = \"./data\"\n",
    "model_path = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForSQuAD(nn.Module):\n",
    "    def __init__(self, bert_model=None):\n",
    "        super(BERTForSQuAD, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.qa_outputs = None\n",
    "        if bert_model:\n",
    "            self.qa_outputs = nn.Linear(bert_model.config.hidden_size, 2, bias=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "        logits = self.qa_outputs(last_hidden_state)  # Shape: [batch_size, sequence_length, 2]\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # Split into start and end logits\n",
    "        start_logits = start_logits.squeeze(-1)  # Shape: [batch_size, sequence_length]\n",
    "        end_logits = end_logits.squeeze(-1)      # Shape: [batch_size, sequence_length]\n",
    "\n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        self.bert.save_pretrained(f\"{save_path}/bert-pod\")\n",
    "        torch.save(self.qa_outputs.state_dict(), f\"{save_path}/linear_adapter.pth\")\n",
    "        torch.save(self.state_dict(), f\"{save_path}/full_model.pth\")\n",
    "    \n",
    "    def load(self, load_path):\n",
    "        self.bert = AutoModel.from_pretrained(f\"{load_path}/bert-pod\")\n",
    "        self.qa_outputs = nn.Linear(self.bert.config.hidden_size, 2, bias=True)\n",
    "        self.qa_outputs.load_state_dict(torch.load(f\"{load_path}/linear_adapter.pth\", weights_only=True))\n",
    "        self.load_state_dict(torch.load(f\"{load_path}/full_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(model, tokenizer, question, context):\n",
    "    # Preprocess\n",
    "    tokenized = tokenizer(question, context, truncation=True)\n",
    "    input_ids = tokenized['input_ids']\n",
    "    token_type_ids = tokenized['token_type_ids']\n",
    "\n",
    "    input_ids_tensor = torch.tensor([input_ids]).to(device)\n",
    "    token_type_ids_tensor = torch.tensor([token_type_ids]).to(device)\n",
    "\n",
    "    # Predict\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    output = model(input_ids_tensor, token_type_ids=token_type_ids_tensor)\n",
    "\n",
    "    answer_start = torch.argmax(output[0])  \n",
    "    answer_end = torch.argmax(output[1])\n",
    "\n",
    "    # Generate answer text\n",
    "    answer_tokens = tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end+1])\n",
    "    answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "    return answer, answer_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_squad_vanilla = BERTForSQuAD()\n",
    "bert_squad_vanilla.load(f\"{model_path}/vanilla_finetuning\")\n",
    "\n",
    "bert_squad_noised = BERTForSQuAD()\n",
    "bert_squad_noised.load(f\"{model_path}/noised_finetuning\")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('csarron/bert-base-uncased-squad-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_answers(question, context):\n",
    "    vanilla_answer, vanilla_start = get_answer(bert_squad_vanilla, tokenizer, question, context)\n",
    "    noised_answer, noised_start = get_answer(bert_squad_noised, tokenizer, question, context)\n",
    "\n",
    "    print(f\"Vanilla training {vanilla_start}: {vanilla_answer}\")\n",
    "    print(f\"Noised training {noised_start}: {noised_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla training 13: bill last night, but bill\n",
      "Noised training 18: \n"
     ]
    }
   ],
   "source": [
    "context = \"Anne called Bill last night, but Bill did not reply.\"\n",
    "question = \"Who did not reply Anne's call?\"\n",
    "\n",
    "model_answers(question, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Simple examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla training 13: apples\n",
      "Noised training 13: apples\n"
     ]
    }
   ],
   "source": [
    "context = \"Anne likes to eat apples.\"\n",
    "question = \"What does Anne likes to eat?\"\n",
    "\n",
    "model_answers(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla training 8: anne\n",
      "Noised training 14: \n"
     ]
    }
   ],
   "source": [
    "context = \"Anne likes to eat apples.\"\n",
    "question = \"Who likes to eat apples?\"\n",
    "\n",
    "model_answers(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla training 13: apples\n",
      "Noised training 13: apples\n"
     ]
    }
   ],
   "source": [
    "context = \"Anne likes to eat apples.\"\n",
    "question = \"What does Anne likes to consume?\"\n",
    "\n",
    "model_answers(question, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Queries not making much sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla training 12: eat cars\n",
      "Noised training 13: cars\n"
     ]
    }
   ],
   "source": [
    "context = \"Anne likes to eat cars.\"\n",
    "question = \"What does Anne likes to consume?\"\n",
    "\n",
    "model_answers(question, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Noised training overfits to \"be\", \"do\", \"has\" and punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla training 21: \n",
      "Noised training 21: zzz\n"
     ]
    }
   ],
   "source": [
    "context = \"zzz is a food. xxx likes to eat zzz. Why yyy has zzz?\"\n",
    "question = \"What does xxx likes to eat?\"\n",
    "\n",
    "model_answers(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla training 21: \n",
      "Noised training 21: zzz why yyy has zzz\n"
     ]
    }
   ],
   "source": [
    "context = \"zzz is a food. xxx likes to eat zzz Why yyy has zzz?\"\n",
    "question = \"What does xxx likes to eat?\"\n",
    "\n",
    "model_answers(question, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
