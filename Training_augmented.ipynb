{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\pytorch-gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Running on {device}')\n",
    "\n",
    "data_path = \"./data\"\n",
    "model_path = \"./models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_jsonlines(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as json_file:\n",
    "        dataset = [json.loads(line) for line in json_file]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Create a wrapper for SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTForSQuAD(nn.Module):\n",
    "    def __init__(self, bert_model=None):\n",
    "        super(BERTForSQuAD, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.qa_outputs = None\n",
    "        if bert_model:\n",
    "            self.qa_outputs = nn.Linear(bert_model.config.hidden_size, 2, bias=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "\n",
    "        logits = self.qa_outputs(last_hidden_state)  # Shape: [batch_size, sequence_length, 2]\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)  # Split into start and end logits\n",
    "        start_logits = start_logits.squeeze(-1)  # Shape: [batch_size, sequence_length]\n",
    "        end_logits = end_logits.squeeze(-1)      # Shape: [batch_size, sequence_length]\n",
    "\n",
    "        return start_logits, end_logits\n",
    "    \n",
    "    def save(self, save_path):\n",
    "        self.bert.save_pretrained(f\"{save_path}/bert-pod\")\n",
    "        torch.save(self.qa_outputs.state_dict(), f\"{save_path}/linear_adapter.pth\")\n",
    "        torch.save(self.state_dict(), f\"{save_path}/full_model.pth\")\n",
    "    \n",
    "    def load(self, load_path):\n",
    "        self.bert = AutoModel.from_pretrained(f\"{load_path}/bert-pod\")\n",
    "        self.qa_outputs = nn.Linear(self.bert.config.hidden_size, 2, bias=True)\n",
    "        self.qa_outputs.load_state_dict(torch.load(f\"{load_path}/linear_adapter.pth\", weights_only=True))\n",
    "        self.load_state_dict(torch.load(f\"{load_path}/full_model.pth\", weights_only=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, AutoModel\n",
    "tokenizer = BertTokenizerFast.from_pretrained('csarron/bert-base-uncased-squad-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Retrieve data in the form of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_lists(squad_train):\n",
    "    question_list = []\n",
    "    context_list = []\n",
    "    answer_start_list = []\n",
    "    answer_end_list = []\n",
    "\n",
    "    for squad_instance in squad_train:\n",
    "        question_list.append(squad_instance[\"question\"])\n",
    "        context_list.append(squad_instance[\"context\"])\n",
    "\n",
    "        answer_start = squad_instance[\"answers\"][\"answer_start\"][0]\n",
    "        answer_end = answer_start + len(squad_instance[\"answers\"][\"text\"][0])\n",
    "        \n",
    "        answer_start_list.append(answer_start)\n",
    "        answer_end_list.append(answer_end)\n",
    "\n",
    "    return question_list, context_list, answer_start_list, answer_end_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Tokenize inputs and convert the starting index of characters to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_squad_data(squad_train, tokenizer):\n",
    "    question_list, context_list, answer_start_list, answer_end_list = retrieve_lists(squad_train)\n",
    "    # Merging the question and answers together\n",
    "    tokenized_data = tokenizer(question_list, context_list, padding = True, truncation = True)\n",
    "\n",
    "    start_token_list = []\n",
    "    end_token_list = []\n",
    "\n",
    "    for i in range(len(squad_train)):\n",
    "    # Skip the question section by setting `sequence_index`=1\n",
    "        start_token = tokenized_data.char_to_token(i, answer_start_list[i], 1)\n",
    "        end_token = tokenized_data.char_to_token(i, answer_end_list[i] - 1, 1)\n",
    "\n",
    "        # if return is None, the answer passage containing the answer is truncated.\n",
    "        if start_token is None:\n",
    "            start_token_list.append(tokenizer.model_max_length-1)\n",
    "        else:\n",
    "            start_token_list.append(start_token)\n",
    "        if end_token is None:\n",
    "            end_token_list.append(tokenizer.model_max_length-1)\n",
    "        else:\n",
    "            end_token_list.append(end_token)\n",
    "        \n",
    "\n",
    "    tokenized_data.update({'answer_start':start_token_list, 'answer_end':end_token_list})\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Create a `Dataset` and convert it to a `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class squad_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_index_data = {}\n",
    "\n",
    "        for key, val in self.tokenized_data.items():\n",
    "            tokenized_index_data.update({key: torch.tensor(val[idx])})\n",
    "\n",
    "        return tokenized_index_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data.answer_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Create a function to create dataloaders from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_from_filename(filename, batch_size):\n",
    "    squad_jsonl = load_from_jsonlines(filename)\n",
    "    tokenized = tokenize_squad_data(squad_jsonl, tokenizer)\n",
    "    dataset = squad_dataset(tokenized)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Compute loss and other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(start_logits, end_logits, answer_start, answer_end):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    start_loss = loss_fn(start_logits, answer_start)\n",
    "    end_loss = loss_fn(end_logits, answer_end)\n",
    "    return (start_loss + end_loss) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(start_logits, end_logits, answer_start, answer_end):\n",
    "    batch_size = start_logits.shape[0]\n",
    "    \n",
    "    pred_start_list = torch.argmax(start_logits, dim=1)\n",
    "    pred_end_list = torch.argmax(end_logits, dim=1)\n",
    "    \n",
    "    F1_list = []\n",
    "    EM_list = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pred_start, pred_end = pred_start_list[i].item(), pred_end_list[i].item()\n",
    "        true_start, true_end = answer_start[i].item(), answer_end[i].item()\n",
    "\n",
    "        # Make sure the answer is valid\n",
    "        if pred_start > pred_end:\n",
    "            pred_end = pred_start\n",
    "\n",
    "        # Calculate F1-score\n",
    "        pred_tokens = set(range(pred_start, pred_end + 1))\n",
    "        true_tokens = set(range(true_start, true_end + 1))\n",
    "\n",
    "        common_tokens = pred_tokens.intersection(true_tokens)\n",
    "\n",
    "        if len(common_tokens) == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            precision = len(common_tokens) / len(pred_tokens)\n",
    "            recall = len(common_tokens) / len(true_tokens)\n",
    "            f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        F1_list.append(f1)\n",
    "\n",
    "        # Calculate Exact Match (EM)\n",
    "        if (pred_start == true_start and pred_end == true_end):\n",
    "            EM_list.append(1)\n",
    "        else:\n",
    "            EM_list.append(0)\n",
    "\n",
    "    # Sum and average later\n",
    "    return F1_list, EM_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Write a function for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    loss_list = []\n",
    "    f1_list = []\n",
    "    em_list = []\n",
    "\n",
    "    for batch in valid_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        answer_start = batch['answer_start'].to(device)\n",
    "        answer_end = batch['answer_end'].to(device)\n",
    "\n",
    "        start_logits, end_logits = model(input_ids, \n",
    "                                         attention_mask=attention_mask, \n",
    "                                         token_type_ids=token_type_ids)\n",
    "        \n",
    "        loss = compute_loss(start_logits, end_logits, answer_start, answer_end)\n",
    "        f1_sublist, em_sublist = compute_metric(start_logits, end_logits, answer_start, answer_end)\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "        f1_list.extend(f1_sublist)\n",
    "        em_list.extend(em_sublist)\n",
    "    \n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    f1 = sum(f1_list) / len(f1_list) * 100\n",
    "    em = sum(em_list) / len(em_list) * 100\n",
    "\n",
    "    return loss, f1, em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train(model, optimizer, train_loader_func, train_schedule, valid_loader, epochs=1, save_path = f\"{model_path}model\"):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    last_valid_loss = torch.inf #\n",
    "    model_to_save = copy.deepcopy(model) # \n",
    "    model_to_save.load_state_dict(model.state_dict()) # \n",
    "    for i in range(epochs):\n",
    "        train_idx = train_schedule[i % len(train_schedule)]\n",
    "        loop = train_loader_func(train_idx)\n",
    "        loss_list = []\n",
    "        for batch in tqdm(loop):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            answer_start = batch['answer_start'].to(device)\n",
    "            answer_end = batch['answer_end'].to(device)\n",
    "\n",
    "            start_logits, end_logits = model(input_ids, \n",
    "                                             attention_mask=attention_mask, \n",
    "                                             token_type_ids=token_type_ids)\n",
    "\n",
    "            loss = compute_loss(start_logits, end_logits, answer_start, answer_end)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "\n",
    "            \n",
    "\n",
    "        train_loss = sum(loss_list) / len(loss_list)\n",
    "        valid_loss, f1, em = valid_model(model, valid_loader)\n",
    "        print(f\"Epoch {i+1}, train_loss: {train_loss:.3f}, valid_loss: {valid_loss:.3f}, EM: {em:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "        if loss < last_valid_loss: #\n",
    "            model_to_save = copy.deepcopy(model) # \n",
    "            model_to_save.load_state_dict(model.state_dict()) # \n",
    "    \n",
    "    # Save model\n",
    "    model_to_save.save(save_path) #\n",
    "\n",
    "    return model_to_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Set up learning rates and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = dataloader_from_filename(f\"{data_path}/squad_valid.jsonl\", batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_squad_noised = BERTForSQuAD()\n",
    "bert_squad_noised.load(f\"{model_path}/vanilla_finetuning\")\n",
    "\n",
    "def train_loader_func(idx):\n",
    "    return dataloader_from_filename(f\"{data_path}/squad_train_noised_{idx}.jsonl\", \n",
    "                                       batch_size=4)\n",
    "\n",
    "train_schedule = [i for i in range(11)] # indices in the train_loader_list\n",
    "\n",
    "learning_rate = 2e-5\n",
    "adam = torch.optim.Adam(bert_squad_noised.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:11<00:00, 87.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 3.499, valid_loss: 2.533, EM: 20.918, F1: 36.290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:28<00:00, 81.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss: 2.681, valid_loss: 2.429, EM: 22.479, F1: 39.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:28<00:00, 81.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train_loss: 2.541, valid_loss: 2.400, EM: 22.696, F1: 40.307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:39<00:00, 78.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train_loss: 2.465, valid_loss: 2.394, EM: 23.926, F1: 40.719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:46<00:00, 76.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, train_loss: 2.413, valid_loss: 2.389, EM: 23.217, F1: 40.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:53<00:00, 74.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, train_loss: 2.365, valid_loss: 2.363, EM: 23.605, F1: 41.703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:46<00:00, 76.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, train_loss: 2.329, valid_loss: 2.393, EM: 22.914, F1: 40.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:58<00:00, 73.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, train_loss: 2.295, valid_loss: 2.409, EM: 23.122, F1: 40.748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [04:52<00:00, 74.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, train_loss: 2.261, valid_loss: 2.423, EM: 23.207, F1: 40.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [05:04<00:00, 71.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train_loss: 2.229, valid_loss: 2.453, EM: 21.996, F1: 39.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21900/21900 [05:04<00:00, 71.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, train_loss: 2.209, valid_loss: 2.483, EM: 22.252, F1: 40.123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTForSQuAD(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_train(model=bert_squad_noised, \n",
    "            optimizer=adam, \n",
    "            train_loader_func=train_loader_func,\n",
    "            train_schedule=train_schedule,\n",
    "            valid_loader=valid_loader,\n",
    "            epochs=11,\n",
    "            save_path=f\"{model_path}/noised_finetuning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
